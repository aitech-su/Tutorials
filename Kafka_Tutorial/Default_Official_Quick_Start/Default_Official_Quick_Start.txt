STEP 1: GET KAFKA
	$ tar -xzf kafka_2.13-3.7.0.tgz
	$ cd kafka_2.13-3.7.0

STEP 2: START THE KAFKA ENVIRONMENT
[[[Kafka with ZooKeeper]]]

# Start the ZooKeeper service
$ bin/zookeeper-server-start.sh config/zookeeper.properties

# Start the Kafka broker service
$ bin/kafka-server-start.sh config/server.properties

[[[Kafka with KRaft]]]

Generate a Cluster UUID
$KAFKA_CLUSTER_ID="$(bin/kafka-storage.sh random-uuid)"

Format Log Directories
$bin/kafka-storage.sh format -t $KAFKA_CLUSTER_ID -c config/kraft/server.properties

Start the Kafka Server
$ bin/kafka-server-start.sh config/kraft/server.properties

[[[Using docker image]]]
$ docker pull apache/kafka:3.7.0
$ docker run -p 9092:9092 apache/kafka:3.7.0

STEP 3: CREATE A TOPIC TO STORE YOUR EVENTS
bin/kafka-topics.sh --create --topic quickstart-events --bootstrap-server localhost:9092
bin/kafka-topics.sh --describe --topic quickstart-events --bootstrap-server localhost:9092

STEP 4: WRITE SOME EVENTS INTO THE TOPIC
$ bin/kafka-console-producer.sh --topic quickstart-events --bootstrap-server localhost:9092
This is my first event
This is my second event

STEP 5: READ THE EVENTS
$ bin/kafka-console-consumer.sh --topic quickstart-events --from-beginning --bootstrap-server localhost:9092
This is my first event
This is my second event

STEP 6: IMPORT/EXPORT YOUR DATA AS STREAMS OF EVENTS WITH KAFKA CONNECT
echo "plugin.path=libs/connect-file-3.7.0.jar" #to config/connect-standalone.properties
echo -e "foo\nbar" > test.txt
> bin/connect-standalone.sh config/connect-standalone.properties config/connect-file-source.properties config/connect-file-sink.properties

During startup you'll see a number of log messages, including some indicating that the connectors are being instantiated. Once the Kafka Connect process has started, the source connector should start reading lines from test.txt and producing them to the topic connect-test, and the sink connector should start reading messages from the topic connect-test and write them to the file test.sink.txt. We can verify the data has been delivered through the entire pipeline by examining the contents of the output file

bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic connect-test --from-beginning
echo baz>> test.txt

STEP 7: PROCESS YOUR EVENTS WITH KAFKA STREAMS